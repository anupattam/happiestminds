{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67356463-10f5-493c-b512-60f054399c95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook ingests raw data from the source system and stores it in the Bronze layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "777572ee-5825-4629-9720-56b07ee11590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Define paths for data layers using Volumes\n",
    "#----------------------------------------------------------------------\n",
    "raw_path = \"/Volumes/workspace/default/raw_data/\"\n",
    "bronze_path = \"/Volumes/workspace/default/bronze/\"\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Import necessary PySpark modules for schema definition and transformations\n",
    "#----------------------------------------------------------------------\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import current_timestamp, to_timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fbabfdf-3de6-4932-b8ce-568d4e109b13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Load Customer Data into Bronze as Delta file\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# 1. Define schema explicitly for better performance (avoid schema inference)\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_unique_id\", StringType(), True),\n",
    "    StructField(\"customer_zip_code_prefix\", StringType(), True),\n",
    "    StructField(\"customer_city\", StringType(), True),\n",
    "    StructField(\"customer_state\", StringType(), True)\n",
    "])\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# 2. Read raw CSV data using the defined schema\n",
    "#--------------------------------------------------------------------------------\n",
    "df_customer = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(customer_schema) \\\n",
    "    .csv(raw_path + \"olist_customers_dataset.csv\")\n",
    "\n",
    "# Optional display\n",
    "#df_customer.display()\n",
    "\n",
    "# --------------------------------------------\n",
    "# 3. Add ingestion timestamp column for lineage and tracking\n",
    "# --------------------------------------------\n",
    "df_customer = df_customer.withColumn(\"created_at\", current_timestamp())\n",
    "\n",
    "# --------------------------------------------\n",
    "# 4. Write to Bronze path in Delta format\n",
    "# mode(\"overwrite\") ensures repeatable testing; has to be changed to \"append\" for prod\n",
    "# partitioning is skipped since customer dataset is small\n",
    "# --------------------------------------------\n",
    "\n",
    "df_customer.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(bronze_path + \"customers\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a1827aa-bbbb-4553-838d-2f07f2558c5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Load Products Data into Bronze as Delta file\n",
    "#--------------------------------------------------------------------------------\n",
    "# 1. Define schema explicitly for better performance (avoid schema inference)\n",
    "#--------------------------------------------------------------------------------\n",
    "product_schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_category_name\", StringType(), True),\n",
    "    StructField(\"product_name_length\", IntegerType(), True),\n",
    "    StructField(\"product_description_length\", IntegerType(), True),\n",
    "    StructField(\"product_photos_qty\", IntegerType(), True),\n",
    "    StructField(\"product_weight_g\", IntegerType(), True),\n",
    "    StructField(\"product_length_cm\", IntegerType(), True),\n",
    "    StructField(\"product_height_cm\", IntegerType(), True),\n",
    "    StructField(\"product_width_cm\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# 2. Read raw CSV data using the defined schema\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "df_product = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(product_schema) \\\n",
    "    .csv(raw_path + \"olist_products_dataset.csv\")\n",
    "\n",
    "# Optional display\n",
    "#df_product.display()\n",
    "\n",
    "# --------------------------------------------\n",
    "# 3. Add ingestion timestamp column for lineage and tracking\n",
    "# --------------------------------------------\n",
    "df_product = df_product.withColumn(\"created_at\", current_timestamp())\n",
    "\n",
    "# --------------------------------------------\n",
    "# Write to Bronze in Delta Format\n",
    "# --------------------------------------------\n",
    "\n",
    "df_product.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(bronze_path + \"products\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef59339-7839-4df8-993e-879ba7a174fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Load Sellers Data into Bronze as Delta file\n",
    "\n",
    "seller_schema = StructType([\n",
    "    StructField(\"seller_id\", StringType(), True),\n",
    "    StructField(\"seller_zip_code_prefix\", StringType(), True),\n",
    "    StructField(\"seller_city\", StringType(), True),\n",
    "    StructField(\"seller_state\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_seller = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(seller_schema) \\\n",
    "    .csv(raw_path + \"olist_sellers_dataset.csv\")\n",
    "\n",
    "# Optional display\n",
    "#df_seller.display()\n",
    "\n",
    "# --------------------------------------------\n",
    "# Add Ingestion Timestamp\n",
    "# --------------------------------------------\n",
    "df_seller = df_seller.withColumn(\"created_at\", current_timestamp())\n",
    "# --------------------------------------------\n",
    "# Write to Bronze in Delta Format\n",
    "# --------------------------------------------\n",
    "\n",
    "df_seller.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(bronze_path + \"sellers\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7aef0cf-3bdc-4468-9f34-f5da96fbc56e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Load Orders Data into Bronze as Delta file\n",
    "# 1. Define schema for the dataset\n",
    "order_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"order_purchase_timestamp\", TimestampType(), True),\n",
    "    StructField(\"order_approved_at\", TimestampType(), True),\n",
    "    StructField(\"order_delivered_carrier_date\", TimestampType(), True),\n",
    "    StructField(\"order_delivered_customer_date\", TimestampType(), True),\n",
    "    StructField(\"order_estimated_delivery_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# 2. Read CSV into DataFrame\n",
    "df_orders = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(order_schema) \\\n",
    "    .csv(raw_path + \"olist_orders_dataset.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# 3. Add ingestion timestamp\n",
    "\n",
    "df_orders = df_orders.withColumn(\"created_at\", current_timestamp())\n",
    "\n",
    "# Optional display\n",
    "#df_orders.display()\n",
    "\n",
    "\n",
    "# 4. Write to Bronze layer in Delta format\n",
    "\n",
    "df_orders.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(bronze_path + \"orders\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a57ac811-b6d5-43dc-b729-b4614512dd88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Load Order reviews Data into Bronze as Delta file\n",
    "# 1. Define schema for the dataset\n",
    "order_reviews_schema = StructType([\n",
    "    StructField(\"review_id\", StringType(), True),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"review_score\", IntegerType(), True),\n",
    "    StructField(\"review_comment_title\", StringType(), True),\n",
    "    StructField(\"review_comment_message\", StringType(), True),\n",
    "    StructField(\"review_creation_date\", TimestampType(), True),\n",
    "    StructField(\"review_answer_timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# 2. Read CSV into DataFrame\n",
    "df_order_reviews = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(order_reviews_schema) \\\n",
    "    .csv(raw_path + \"olist_order_reviews_dataset.csv\")\n",
    "\n",
    "\n",
    "# 3. Add ingestion timestamp\n",
    "df_order_reviews = df_order_reviews.withColumn(\"created_at\", current_timestamp())\n",
    "\n",
    "\n",
    "# Optional display\n",
    "#df_order_reviews.display()\n",
    "\n",
    "# 4. Write to Bronze layer in Delta format\n",
    "df_order_reviews.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(bronze_path + \"orders_reviews\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45d99711-ab6e-407c-8f16-445da0798389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. Load Order items Data into Bronze as Delta file\n",
    "\n",
    "# 1. Define schema for the dataset \n",
    "order_items_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"order_item_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"seller_id\", StringType(), True),\n",
    "    StructField(\"shipping_limit_date\", TimestampType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"freight_value\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# 2. Read CSV into DataFrame\n",
    "df_order_items = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(order_items_schema) \\\n",
    "    .csv(raw_path + \"olist_order_items_dataset.csv\")\n",
    "\n",
    "\n",
    "# 3. Add ingestion timestamp\n",
    "df_order_items = df_order_items.withColumn(\"created_at\", current_timestamp())\n",
    "\n",
    "# Optional display\n",
    "#df_order_items.display()\n",
    "\n",
    "# 4. Write to Bronze layer in Delta format\n",
    "\n",
    "df_order_items.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(bronze_path + \"orders_items\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "289d80d3-cd10-4553-927e-d22c355ae41f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7. Load Order Payments Data into Bronze as Delta file\n",
    "\n",
    "# 1. Define schema for the dataset \n",
    "order_payments_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"payment_sequential\", IntegerType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"payment_installments\", IntegerType(), True),\n",
    "    StructField(\"payment_value\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# 2. Read CSV into DataFrame\n",
    "df_order_payments = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(order_payments_schema) \\\n",
    "    .csv(raw_path + \"olist_order_items_dataset.csv\")\n",
    "\n",
    "\n",
    "# 3. Add ingestion timestamp\n",
    "df_order_payments = df_order_payments.withColumn(\"created_at\", current_timestamp())\n",
    "\n",
    "# Optional display\n",
    "#df_order_payments.display()\n",
    "\n",
    "# 4. Write to Bronze layer in Delta format\n",
    "df_order_payments.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(bronze_path + \"order_payments\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f3e734a-433e-404e-809b-20d902189389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 8. Load geolocation Data into Bronze as Delta file\n",
    "\n",
    "# 1. Define schema for the dataset\n",
    "geolocation_schema = StructType([\n",
    "    StructField(\"geolocation_zip_code_prefix\", StringType(), True),\n",
    "    StructField(\"geolocation_lat\", DoubleType(), True),\n",
    "    StructField(\"geolocation_lng\", DoubleType(), True),\n",
    "    StructField(\"geolocation_city\", StringType(), True),\n",
    "    StructField(\"geolocation_state\", StringType(), True)\n",
    "])\n",
    "\n",
    "# 2. Read CSV into DataFrame\n",
    "df_geolocation = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(geolocation_schema) \\\n",
    "    .csv(raw_path + \"olist_geolocation_dataset.csv\")\n",
    "\n",
    "# 3. Add ingestion timestamp\n",
    "df_geolocation = df_geolocation.withColumn(\"created_at\", current_timestamp())\n",
    "\n",
    "# Optional display\n",
    "#df_geolocation.display()\n",
    "\n",
    "# 4. Write to Bronze layer in Delta format\n",
    "df_geolocation.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(bronze_path + \"geolocation\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f4f4296-ac25-4250-b06c-406268c4e929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 9. Load Product Category Name translation Data into Bronze as Delta file\n",
    "\n",
    "# 1. Define schema for the dataset\n",
    "product_category_schema = StructType([\n",
    "    StructField(\"product_category_name\", StringType(), True),\n",
    "    StructField(\"product_category_name_english\", StringType(), True)\n",
    "])\n",
    "\n",
    "# 2. Read CSV into DataFrame\n",
    "df_product_category = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(product_category_schema) \\\n",
    "    .csv(raw_path + \"product_category_name_translation.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# 3. Add ingestion timestamp\n",
    "df_product_category = df_product_category.withColumn(\"created_at\", current_timestamp())\n",
    "\n",
    "# Optional display\n",
    "#df_product_category.display()\n",
    "\n",
    "# 4. Write to Bronze layer in Delta format\n",
    "df_product_category.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(bronze_path + \"product_category\") \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest_source_to_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
