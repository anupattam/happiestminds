{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "879a3807-14fa-4da6-890d-0135580ea14c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Define paths for data layers using Volumes\n",
    "#----------------------------------------------------------------------\n",
    "bronze_path = \"/Volumes/workspace/default/bronze/\"\n",
    "silver_path = \"/Volumes/workspace/default/silver/\"\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Import required functions and libraries\n",
    "#----------------------------------------------------------------------\n",
    "from pyspark.sql.functions import udf, current_timestamp, col, broadcast, when\n",
    "from pyspark.sql.types import StringType\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Define a UDF to remove accents from strings (e.g., for cleaning text fields)\n",
    "remove_accents_udf = udf(lambda s: unidecode(s) if s else None, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b65fc9-d9ab-44e1-b7a3-bf5489b2896b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------\n",
    "# Promote product category translation data from Bronze to Silver with metadata\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# 1. Read from Bronze Delta table\n",
    "df_product_category = spark.read.format(\"delta\").load(bronze_path + \"product_category\")\n",
    "\n",
    "# 2. Add processed timestamp column for audit lineage\n",
    "df_product_category = df_product_category.withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "# Optional display\n",
    "#df_product_category.display()\n",
    "\n",
    "# 3. Write to Silver Delta table\n",
    "df_product_category.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(silver_path + \"product_category\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad36af3-70d5-4c14-9190-a4a77ddb29cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------\n",
    "# Clean and Transform Products Dataset → Promote to Silver\n",
    "# ----------------------------------------------------------------------------------------\n",
    "\n",
    "# 1. Read Bronze Delta table\n",
    "df_products = spark.read.format(\"delta\").load(bronze_path + \"products\")\n",
    "\n",
    "# 2. Fill nulls with sensible defaults for known columns\n",
    "df_products = df_products.fillna({\n",
    "    \"product_category_name\": \"N/A\",\n",
    "    \"product_name_length\": 0,\n",
    "    \"product_description_length\": 0,\n",
    "    \"product_photos_qty\": 0\n",
    "})\n",
    "\n",
    "# 3. Join with translated category names (broadcast for performance)\n",
    "df_joined = df_products.join( \\\n",
    "    broadcast(df_product_category),\\\n",
    "    on=\"product_category_name\", \\\n",
    "    how=\"left\")\n",
    "\n",
    "# 4. Clean up columns:\n",
    "#    - Drop original (non-English) category\n",
    "#    - Rename English name to unified column\n",
    "df_final = df_joined \\\n",
    "   .drop(df_product_category[\"created_at\"])\\\n",
    "   .drop(df_joined[\"product_category_name\"]) \\\n",
    "   .withColumnRenamed(\"product_category_name_english\", \"product_category_name\")\n",
    "\n",
    "# 5. Normalize product category names with replacements\n",
    "df_final = df_final.withColumn(\n",
    "\"product_category_name\",\n",
    "when(col(\"product_category_name\") == \"pc_gamer\", \"gaming_pc\")\n",
    ".when(col(\"product_category_name\") == \"portateis_cozinha_e_preparadores_de_alimentos\", \"portable_kitchen_and_food_preparators\")\n",
    ".when(col(\"product_category_name\").isNull(), \"N/A\")\n",
    ".otherwise(col(\"product_category_name\"))\n",
    ")\n",
    "\n",
    "df_products = df_final\n",
    "\n",
    "# 6. Add processed timestamp for audit trail\n",
    "df_products = df_products.withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "# 7. Write cleaned data to Silver Delta table\n",
    "df_products.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(silver_path + \"products\") \n",
    "\n",
    "# Optional display\n",
    "# df_products.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24e26942-093a-4b6c-9fa9-f375e838cf3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------\n",
    "# Promote Customers data from Bronze to Silver with metadata\n",
    "#--------------------------------------------------------------------------------------------\n",
    "df_customer = spark.read.format(\"delta\").load(bronze_path + \"customers\")\n",
    "\n",
    "# Add processed timestamp for audit trail\n",
    "df_customer = df_customer.withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "# Optional display\n",
    "#df_customer.display()\n",
    "\n",
    "# Write cleaned data to Silver Delta table\n",
    "\n",
    "df_customer.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(silver_path + \"customers\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07983fc3-4735-433f-960f-c7aae0013ff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------\n",
    "# Promote Sellers data from Bronze to Silver with metadata\n",
    "#--------------------------------------------------------------------------------------------\n",
    "\n",
    "df_seller = spark.read.format(\"delta\").load(bronze_path + \"sellers\")\n",
    "\n",
    "# Add processed timestamp for audit trail\n",
    "df_seller = df_seller.withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "# Optional display\n",
    "#df_seller.display()\n",
    "\n",
    "# Write cleaned data to Silver Delta table\n",
    "df_seller.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(silver_path + \"sellers\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c645881-53f5-4e66-8574-12fa3e22559a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------\n",
    "# Promote Orders data from Bronze to Silver with metadata\n",
    "#--------------------------------------------------------------------------------------------\n",
    "# Read from Bronze Delta table\n",
    "df_orders = spark.read.format(\"delta\").load(bronze_path + \"orders\")\n",
    "\n",
    "# Add processed timestamp for audit trail\n",
    "df_orders = df_orders.withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "# Optional display\n",
    "#df_orders.display()\n",
    "\n",
    "# Write cleaned data to Silver Delta table\n",
    "df_orders.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(silver_path + \"orders\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "680a06e1-a414-4e94-bf64-27e0a463adb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------\n",
    "# Transform Order Reviews Dataset → Clean Accents and Promote to Silver\n",
    "# --------------------------------------------------------------------------------------------\n",
    "# 1. Read from Bronze Delta table\n",
    "df_orders_reviews = spark.read.format(\"delta\").load(bronze_path + \"orders_reviews\")\n",
    "\n",
    "# 2. Remove accents from textual fields using UDF (defined earlier)\n",
    "df_orders_reviews = df_orders_reviews\\\n",
    "    .withColumn(\"review_comment_title\", remove_accents_udf(col(\"review_comment_title\")))\\\n",
    "    .withColumn(\"review_comment_message\", remove_accents_udf(col(\"review_comment_message\")))\n",
    "\n",
    "\n",
    "# 3. Add processed timestamp for audit tracking\n",
    "df_orders_reviews = df_orders_reviews.withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "# Optional display\n",
    "#df_orders_reviews.display()\n",
    "\n",
    "# 4. Write the cleaned data to Silver layer, consider 'merge' or 'append' for production workloads\n",
    "df_orders_reviews.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(silver_path + \"orders_reviews\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19df02ba-cc81-4dac-bcad-dc1d74977ce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------\n",
    "# Promote Order items data from Bronze to Silver with metadata\n",
    "#--------------------------------------------------------------------------------------------\n",
    "# Read from Bronze Delta table\n",
    "df_orders_items = spark.read.format(\"delta\").load(bronze_path + \"orders_items\")\n",
    "\n",
    "# Add processed timestamp for audit trail\n",
    "df_orders_items = df_orders_items.withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "# Optional display\n",
    "#df_orders_items.display()\n",
    "\n",
    "# Write cleaned data to Silver Delta table\n",
    "\n",
    "df_orders_items.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(silver_path + \"orders_items\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfd2633b-a41a-43cd-b447-29b1dc494e21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------\n",
    "# Promote Order Payments data from Bronze to Silver with metadata\n",
    "#--------------------------------------------------------------------------------------------\n",
    "# 1. Read from Bronze Delta table\n",
    "df_order_payments = spark.read.format(\"delta\").load(bronze_path + \"order_payments\")\n",
    "\n",
    "# Add processed timestamp for audit trail\n",
    "df_order_payments = df_order_payments.withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "# Optional display\n",
    "#df_order_payments.display()\n",
    "\n",
    "# Write cleaned data to Silver Delta table\n",
    "df_order_payments.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(silver_path + \"order_payments\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3294d85-aab4-4713-b7d3-b6613f038950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------\n",
    "# Promote geolocation data from bronze to Silver with metadata\n",
    "#--------------------------------------------------------------------------------------------\n",
    "# 1. Read from Bronze Delta table\n",
    "df_geolocation = spark.read.format(\"delta\").load(bronze_path + \"geolocation\")\n",
    "\n",
    "df_geolocation = df_geolocation.withColumn(\"geolocation_city\", remove_accents_udf(col(\"geolocation_city\")))\n",
    "\n",
    "# Add processed timestamp for audit trail\n",
    "df_geolocation = df_geolocation.withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "# Optional display\n",
    "#df_geolocation.select(\"geolocation_city\").distinct().display()\n",
    "\n",
    "# Write cleaned data to Silver Delta table\n",
    "df_geolocation.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(silver_path + \"geolocation\") "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "clean_bronze_copy_to_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
